{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "ddd = pd.read_csv('csvs/linkedin_f.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3741507411"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddd.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appended data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>source</th>\n",
       "      <th>min_yearly_salary</th>\n",
       "      <th>max_yearly_salary</th>\n",
       "      <th>company_link</th>\n",
       "      <th>description</th>\n",
       "      <th>python</th>\n",
       "      <th>sql</th>\n",
       "      <th>r</th>\n",
       "      <th>scala</th>\n",
       "      <th>...</th>\n",
       "      <th>mysql</th>\n",
       "      <th>postgresql</th>\n",
       "      <th>nosql</th>\n",
       "      <th>etl</th>\n",
       "      <th>dax</th>\n",
       "      <th>aws</th>\n",
       "      <th>azure</th>\n",
       "      <th>junior</th>\n",
       "      <th>mid</th>\n",
       "      <th>senior</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3741644335</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>79300.0</td>\n",
       "      <td>https://uk.linkedin.com/company/harnham?trk=pu...</td>\n",
       "      <td>\\nTo Apply for this Job Click HereGEOSPATIAL D...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID    source  min_yearly_salary  max_yearly_salary  \\\n",
       "0  3741644335  LinkedIn            61000.0            79300.0   \n",
       "\n",
       "                                        company_link  \\\n",
       "0  https://uk.linkedin.com/company/harnham?trk=pu...   \n",
       "\n",
       "                                         description  python    sql     r  \\\n",
       "0  \\nTo Apply for this Job Click HereGEOSPATIAL D...    True  False  True   \n",
       "\n",
       "   scala  ...  mysql  postgresql  nosql    etl    dax    aws  azure  junior  \\\n",
       "0  False  ...  False       False  False  False  False  False  False   False   \n",
       "\n",
       "     mid  senior  \n",
       "0  False    True  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "from webscraping import By, find_technologies_in_string, linkedin_get_salary\n",
    "from time import sleep\n",
    "data_list = []\n",
    "for i in range(len(ddd)):\n",
    "    SOURCE = 'LinkedIn'\n",
    "    ID = ddd.iloc[i,0]\n",
    "    url = ddd.iloc[i,-2]\n",
    "    # print(url)\n",
    "    r = requests.get(url)\n",
    "    s = bs(r.text, 'html')\n",
    "    digest = s.text\n",
    "    job_description = s.find_all('div',class_=\"show-more-less-html__markup\")[0].text\n",
    "    # sleep(3)\n",
    "    salary = linkedin_get_salary(digest)\n",
    "    if len(salary)>1:\n",
    "        lower = salary[0]\n",
    "        higher = salary[1]\n",
    "    elif len(salary) == 1:\n",
    "        lower = salary[0]\n",
    "        higher = salary[0]\n",
    "    else:\n",
    "        lower = None\n",
    "        higher = None\n",
    "    techs = find_technologies_in_string(digest)\n",
    "    # data\n",
    "    link = s.find_all(\"a\", attrs={\"data-tracking-control-name\":\"public_jobs_topcard_logo\"})[0]\n",
    "    link = link.get(\"href\")\n",
    "    data = {'ID':ID, 'source':SOURCE, 'min_yearly_salary':lower,\n",
    "                'max_yearly_salary':higher,'company_link':link,'description':job_description}\n",
    "    data.update(techs)\n",
    "    data_list.append(data)\n",
    "    print('appended data')\n",
    "df_new = pd.DataFrame(data_list)\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webscraping import linkedin_page_go_to,linkedin_get_text_values_by_class,linkedin_get_href_by_class\n",
    "from webscraping import linkedin_get_href_by_class,linkedin_driver, linkedin_get_elements_by_css\n",
    "from time import sleep\n",
    "from webscraping import linkedin_job_find_next_page_button, linkedin_click\n",
    "from webscraping import By, find_technologies_in_string\n",
    "from random import randint\n",
    "\n",
    "data_list = []\n",
    "counter = 0\n",
    "for row in ddd.iterrows():\n",
    "    counter+=1\n",
    "    if counter>10:\n",
    "        break\n",
    "    ID = row[1][0]\n",
    "    SOURCE = row[1][-1]\n",
    "    # print(ID)\n",
    "    linkedin_page_go_to(driver, row[1][5])\n",
    "    sleep(randint(1,2))\n",
    "    # sleep(1)\n",
    "    try:\n",
    "        lower,higher = None , None\n",
    "        button = driver.find_element(By.CLASS_NAME, 'jobs-description__footer-button')\n",
    "        linkedin_click(button)\n",
    "        # print('clicking btton')\n",
    "        description = driver.find_elements(By.CLASS_NAME,'jobs-box__html-content')[0].text\n",
    "        print('found elements')\n",
    "        link = linkedin_get_href_by_class('app-aware-link',driver)\n",
    "        print('found link')\n",
    "        salary = driver.find_element(By.CLASS_NAME, 'job-details-jobs-unified-top-card__job-insight')\n",
    "        spans = salary.find_elements(By.TAG_NAME, 'span')\n",
    "        sal = spans[0].text\n",
    "        if sal[0] =='£':\n",
    "            lower  = int(int((sal.split(' ')[0].split('/')[0][1:]).replace(',', ''))*1.22)\n",
    "            higher = int(int((sal.split(' ')[2].split('/')[0][1:]).replace(',', ''))*1.22)\n",
    "        elif sal[0] =='$':\n",
    "            lower  = int(int((sal.split(' ')[0].split('/')[0][1:]).replace(',', '')))\n",
    "            higher = int(int((sal.split(' ')[2].split('/')[0][1:]).replace(',', '')))\n",
    "        else:\n",
    "            pass\n",
    "        data = {'ID':ID, 'source':SOURCE, 'min_yearly_salary':lower,\n",
    "                'max_yearly_salary':higher,'company_link':link,'description':description}\n",
    "        techs = find_technologies_in_string(description)\n",
    "        data.update(techs)\n",
    "        data_list.append(data)\n",
    "        print('appended data')\n",
    "    except:\n",
    "        pass\n",
    "df_new = pd.DataFrame(data_list)\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary = driver.find_element(By.CLASS_NAME, 'job-details-jobs-unified-top-card__job-insight')\n",
    "spans = salary.find_elements(By.TAG_NAME, 'span')\n",
    "sal = spans[0].text\n",
    "if sal[0] =='£':\n",
    "    print(int(int((sal.split(' ')[0].split('/')[0][1:]).replace(',', ''))*1.22))\n",
    "    print(int(int((sal.split(' ')[2].split('/')[0][1:]).replace(',', ''))*1.22))\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# The URL containing the JSON response\n",
    "url =  \"https://github.com/clarketm/proxy-list/blob/master/proxy-list-raw.txt\"\n",
    "\n",
    "# Send a GET request to fetch the JSON response\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    data = json.loads(response.text)\n",
    "\n",
    "    # Extract the proxy addresses from the JSON data\n",
    "    proxy_data = data.get(\"payload\", {}).get(\"blob\", {}).get(\"rawLines\", [])\n",
    "    \n",
    "    # Print the proxy addresses\n",
    "    for proxy in proxy_data:\n",
    "        print(proxy)\n",
    "else:\n",
    "    print(\"Failed to fetch the JSON data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
