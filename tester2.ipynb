{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "ddd = pd.read_csv('csvs/linkedin_f.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddd.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "from webscraping import By, find_technologies_in_string, linkedin_get_salary\n",
    "from time import sleep\n",
    "from random import randint\n",
    "data_list = []\n",
    "for i in range(len(ddd)):\n",
    "    try:\n",
    "        SOURCE = 'LinkedIn'\n",
    "        ID = ddd.iloc[i,0]\n",
    "        url = ddd.iloc[i,-2]\n",
    "        # print(url)\n",
    "        r = requests.get(url)\n",
    "        s = bs(r.text, 'html')\n",
    "        digest = s.text\n",
    "        job_description = s.find_all('div',class_=\"show-more-less-html__markup\")[0].text\n",
    "        sleep(randint(1,4))\n",
    "        salary = linkedin_get_salary(digest)\n",
    "        if len(salary)>1:\n",
    "            lower = salary[0]\n",
    "            higher = salary[1]\n",
    "        elif len(salary) == 1:\n",
    "            lower = salary[0]\n",
    "            higher = salary[0]\n",
    "        else:\n",
    "            lower = None\n",
    "            higher = None\n",
    "        techs = find_technologies_in_string(digest)\n",
    "        # data\n",
    "        link = s.find_all(\"a\", attrs={\"data-tracking-control-name\":\"public_jobs_topcard_logo\"})[0]\n",
    "        link = link.get(\"href\")\n",
    "        data = {'ID':ID, 'source':SOURCE, 'min_yearly_salary':lower,\n",
    "                    'max_yearly_salary':higher,'company_link':link,'description':job_description}\n",
    "        data.update(techs)\n",
    "        data_list.append(data)\n",
    "        print('appended data')\n",
    "    except:\n",
    "        pass\n",
    "df_new = pd.DataFrame(data_list)\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webscraping import linkedin_page_go_to,linkedin_get_text_values_by_class,linkedin_get_href_by_class\n",
    "from webscraping import linkedin_get_href_by_class,linkedin_driver, linkedin_get_elements_by_css\n",
    "from time import sleep\n",
    "from webscraping import linkedin_job_find_next_page_button, linkedin_click\n",
    "from webscraping import By, find_technologies_in_string\n",
    "from random import randint\n",
    "\n",
    "data_list = []\n",
    "counter = 0\n",
    "for row in ddd.iterrows():\n",
    "    counter+=1\n",
    "    if counter>10:\n",
    "        break\n",
    "    ID = row[1][0]\n",
    "    SOURCE = row[1][-1]\n",
    "    # print(ID)\n",
    "    linkedin_page_go_to(driver, row[1][5])\n",
    "    sleep(randint(1,2))\n",
    "    # sleep(1)\n",
    "    try:\n",
    "        lower,higher = None , None\n",
    "        button = driver.find_element(By.CLASS_NAME, 'jobs-description__footer-button')\n",
    "        linkedin_click(button)\n",
    "        # print('clicking btton')\n",
    "        description = driver.find_elements(By.CLASS_NAME,'jobs-box__html-content')[0].text\n",
    "        print('found elements')\n",
    "        link = linkedin_get_href_by_class('app-aware-link',driver)\n",
    "        print('found link')\n",
    "        salary = driver.find_element(By.CLASS_NAME, 'job-details-jobs-unified-top-card__job-insight')\n",
    "        spans = salary.find_elements(By.TAG_NAME, 'span')\n",
    "        sal = spans[0].text\n",
    "        if sal[0] =='£':\n",
    "            lower  = int(int((sal.split(' ')[0].split('/')[0][1:]).replace(',', ''))*1.22)\n",
    "            higher = int(int((sal.split(' ')[2].split('/')[0][1:]).replace(',', ''))*1.22)\n",
    "        elif sal[0] =='$':\n",
    "            lower  = int(int((sal.split(' ')[0].split('/')[0][1:]).replace(',', '')))\n",
    "            higher = int(int((sal.split(' ')[2].split('/')[0][1:]).replace(',', '')))\n",
    "        else:\n",
    "            pass\n",
    "        data = {'ID':ID, 'source':SOURCE, 'min_yearly_salary':lower,\n",
    "                'max_yearly_salary':higher,'company_link':link,'description':description}\n",
    "        techs = find_technologies_in_string(description)\n",
    "        data.update(techs)\n",
    "        data_list.append(data)\n",
    "        print('appended data')\n",
    "    except:\n",
    "        pass\n",
    "df_new = pd.DataFrame(data_list)\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary = driver.find_element(By.CLASS_NAME, 'job-details-jobs-unified-top-card__job-insight')\n",
    "spans = salary.find_elements(By.TAG_NAME, 'span')\n",
    "sal = spans[0].text\n",
    "if sal[0] =='£':\n",
    "    print(int(int((sal.split(' ')[0].split('/')[0][1:]).replace(',', ''))*1.22))\n",
    "    print(int(int((sal.split(' ')[2].split('/')[0][1:]).replace(',', ''))*1.22))\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# The URL containing the JSON response\n",
    "url =  \"https://github.com/clarketm/proxy-list/blob/master/proxy-list-raw.txt\"\n",
    "\n",
    "# Send a GET request to fetch the JSON response\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    data = json.loads(response.text)\n",
    "\n",
    "    # Extract the proxy addresses from the JSON data\n",
    "    proxy_data = data.get(\"payload\", {}).get(\"blob\", {}).get(\"rawLines\", [])\n",
    "    \n",
    "    # Print the proxy addresses\n",
    "    for proxy in proxy_data:\n",
    "        print(proxy)\n",
    "else:\n",
    "    print(\"Failed to fetch the JSON data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
